{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69a2725-aae5-4e7f-8efa-8ed1fa40a3fb",
   "metadata": {},
   "source": [
    "## Import and Class Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3639c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "mujoco_safety_gymnasium_dict = {\n",
    "    'SafetyAntVelocity-v1': {'state_dim': 27, 'action_dim': 8, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyHalfCheetahVelocity-v1': {'state_dim': 17, 'action_dim': 6, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyHopperVelocity-v1': {'state_dim': 11, 'action_dim': 3, 'decoder_arch': [64, 64],}, \n",
    "    'SafetySwimmerVelocity-v1': {'state_dim': 8, 'action_dim': 2, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyWalker2dVelocity-v1': {'state_dim': 17, 'action_dim': 6, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyPointCircle1-v0': {'state_dim': 28, 'action_dim': 2, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyPointCircle2-v0': {'state_dim': 28, 'action_dim': 2, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyCarCircle1-v0': {'state_dim': 40, 'action_dim': 2, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyCarCircle2-v0': {'state_dim': 40, 'action_dim': 2, 'decoder_arch': [64, 64],}, \n",
    "    'SafetyAntRun-v0': {'state_dim': 33, 'action_dim': 8, 'decoder_arch': [64, 64],},\n",
    "    'SafetyBallRun-v0': {'state_dim': 7, 'action_dim': 2, 'decoder_arch': [64, 64],},\n",
    "    'SafetyCarRun-v0': {'state_dim': 7, 'action_dim': 2, 'decoder_arch': [64, 64],},\n",
    "    'SafetyDroneRun-v0': {'state_dim': 17, 'action_dim': 4, 'decoder_arch': [64, 64],},    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b740674-1168-4a07-a729-6b4962dd61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import LogNormal  # , Normal\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import datetime\n",
    "\n",
    "LOC_MAX = 3\n",
    "LOC_MIN = -20\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "MIN_LOGSCORE = -7\n",
    "\n",
    "class PtEstGRU(nn.Module):\n",
    "    def __init__(self, feature_dim=11, nb_gru_units=16, batch_size=256, gru_layers=2, mlp_arch=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        if mlp_arch is None:\n",
    "            mlp_arch = [64, 64]\n",
    "        self.hidden = None\n",
    "        self.feature_dim = feature_dim\n",
    "        self.nb_gru_units = nb_gru_units\n",
    "        self.gru_layers = gru_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.mlp_arch = mlp_arch\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "\n",
    "    def __build_model(self):\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.nb_gru_units,\n",
    "            num_layers=self.gru_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # Decoder Module\n",
    "        # self.sa_embedding = nn.Identity()\n",
    "        decoder = []\n",
    "        # prev_in_features = self.feature_dim + self.nb_gru_units\n",
    "        prev_in_features = self.nb_gru_units * 2\n",
    "        for out_features in self.mlp_arch:\n",
    "            decoder.append(nn.Linear(prev_in_features, out_features))\n",
    "            decoder.append(nn.ReLU())\n",
    "            # decoder.append(nn.LayerNorm(out_features))\n",
    "            prev_in_features = out_features\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "\n",
    "        self.decoder_output = nn.Linear(self.mlp_arch[-1], 1)\n",
    "        nn.init.normal_(self.decoder_output.weight, mean=-0.5, std=0.1)\n",
    "        nn.init.constant_(self.decoder_output.bias, -5.0)\n",
    "\n",
    "    def init_hidden(self, init_h=None):\n",
    "\n",
    "        if init_h is None:\n",
    "            # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "            hidden_h = th.zeros(self.gru_layers, self.batch_size, self.nb_gru_units)\n",
    "        else:\n",
    "            hidden_h = init_h\n",
    "\n",
    "        hidden_h = Variable(hidden_h)\n",
    "\n",
    "        return hidden_h\n",
    "\n",
    "    def forward(self, x, x_lengths, init_h=None):\n",
    "        # reset the hidden state. Must be done before you run a new batch\n",
    "        self.hidden = self.init_hidden(init_h)\n",
    "        # print(self.hidden)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = x.size()\n",
    "        x_clone = x.clone().swapaxes(0, 1)\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the GRU\n",
    "        x = th.nn.utils.rnn.pack_padded_sequence(x, x_lengths.cpu(), enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        # now run through GRU\n",
    "        x, self.hidden = self.gru(x, self.hidden)\n",
    "        x_unpack = th.nn.utils.rnn.pad_packed_sequence(x, batch_first=False, padding_value=0.0)\n",
    "        # print(torch.exp((-nn.ReLU()(self.out(X_unpack[0]))).sum(dim=0)))\n",
    "        # print(X_unpack[0][:-1].shape)\n",
    "\n",
    "        if init_h is None:\n",
    "            h0 = th.zeros(1, batch_size, self.nb_gru_units)  # self.batch, Not batch_size\n",
    "        else:\n",
    "            h0 = init_h[-1:]\n",
    "\n",
    "        # combinedH = th.cat((h0, x_unpack[0][:-1]), 0)\n",
    "        h_t_vector = th.cat((h0, x_unpack[0][:-1]), 0)\n",
    "        h_tplusone_vector = x_unpack[0]\n",
    "\n",
    "        # sa_embed = self.sa_embedding(x_clone)\n",
    "        # combinedSAH = th.cat((sa_embed, combinedH), -1)\n",
    "        combined_two_h = th.cat((h_t_vector, h_tplusone_vector), -1)\n",
    "\n",
    "        # log_scores, log_scores_mean, log_scores_variance = self._calc_logscores(combinedSAH)\n",
    "        log_scores, log_scores_mean, log_scores_variance = self._calc_logscores(combined_two_h)\n",
    "\n",
    "        y_hat = th.exp(log_scores.sum(dim=0))\n",
    "        # meanH = X_unpack[0].sum(dim=0) / X_unpack[1][:, None].to(device)\n",
    "        # y_hat = self.class_output(meanH)\n",
    "\n",
    "        # predicted probability, log C with shape [T, B, 1] (mean and variance)\n",
    "        return y_hat, {'log_scores': log_scores, 'mean': log_scores_mean, 'var': log_scores_variance}, x_unpack[0], self.hidden\n",
    "\n",
    "    def forward_loss_metrics(self, x, y, x_lengths):\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, dict_log_c_out, h_out, _ = self(x, x_lengths)\n",
    "        loss = self.loss(outputs, y.float())\n",
    "\n",
    "        y = y.bool()\n",
    "        correct = ((outputs > 0.5) == y).sum().item()\n",
    "        tp = ((outputs > 0.5) & y).sum().item()\n",
    "        fp = ((outputs > 0.5) & y.logical_not()).sum().item()\n",
    "        tn = ((outputs <= 0.5) & y.logical_not()).sum().item()\n",
    "        fn = ((outputs <= 0.5) & y).sum().item()\n",
    "\n",
    "        return loss, correct, tp, fp, tn, fn\n",
    "\n",
    "    def _calc_logscores(self, concat_two_h):\n",
    "        log_scores = -nn.ReLU()(self.decoder_output(self.decoder(concat_two_h)))\n",
    "        # return log_scores, mean, variance\n",
    "        return th.clamp(log_scores, MIN_LOGSCORE, 0), None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_hat, y, classweight=1.):\n",
    "\n",
    "        loss = nn.BCELoss()\n",
    "        loss = loss(y_hat, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "        # bce_loss = nn.BCELoss(reduction='none')\n",
    "        # interim_loss = bce_loss(y_hat, y)\n",
    "        # # weights = torch.ones_like(y) + (y == 0).float()\n",
    "        # class_zero_weight = classweight / (classweight + 1)\n",
    "        # class_one_weight = 1 - class_zero_weight\n",
    "        # weights = th.zeros_like(y) + class_zero_weight * (y == 0) + class_one_weight * (y == 1)\n",
    "        #\n",
    "        # # print(\"y\")\n",
    "        # # print(y)\n",
    "        # # print(\"weights\")\n",
    "        # # print(weights)\n",
    "        #\n",
    "        # return th.mean(2 * weights * interim_loss)\n",
    "\n",
    "\n",
    "class DistributionGRU(PtEstGRU):\n",
    "    def __init__(self, feature_dim=11, nb_gru_units=16, batch_size=256, gru_layers=2, mlp_arch=None, dropout=0.0,\n",
    "                 loc_offset=0.0, log_std_offset=0.0):\n",
    "        super().__init__(feature_dim, nb_gru_units, batch_size, gru_layers, mlp_arch, dropout)\n",
    "        # self.decoder_output = nn.Linear(self.mlp_arch[-1], 2)\n",
    "        self.decoder_output_logstd = nn.Linear(self.mlp_arch[-1], 1)\n",
    "        self.loc_offset = loc_offset\n",
    "        self.log_std_offset = log_std_offset\n",
    "\n",
    "    # def __build_model(self):\n",
    "    #     print(\"Dist GRU build model\")\n",
    "    #     super().__build_model()\n",
    "    #     self.decoder_output = nn.Linear(256, 2)\n",
    "    #     print(\"Completed dist GRU build model\")\n",
    "\n",
    "    def _calc_logscores(self, concat_two_h):\n",
    "        # [T, B, 2]\n",
    "        loc_params = self.decoder_output(self.decoder(concat_two_h)) - self.loc_offset\n",
    "        loc_params = th.clamp(loc_params, LOC_MIN, LOC_MAX)\n",
    "\n",
    "        log_std_params = self.decoder_output_logstd(self.decoder(concat_two_h)) - self.log_std_offset\n",
    "        log_std_params = th.clamp(log_std_params, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        score_std = th.ones_like(loc_params) * log_std_params.exp()\n",
    "        distributions = LogNormal(loc_params, score_std)\n",
    "        log_scores = -distributions.rsample()  # [T, B]\n",
    "\n",
    "        # return th.clamp(log_scores.unsqueeze(-1), MIN_LOGSCORE, 0), -distributions.mean, distributions.variance\n",
    "        return th.clamp(log_scores, MIN_LOGSCORE, 0), -distributions.mean, distributions.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579256f-1ac9-413e-af19-0b02329249a7",
   "metadata": {},
   "source": [
    "## Env and Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b4a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'SafetyCarCircle2-v0'\n",
    "base_path = '/SSD2/siowmeng/icml25_results/save_traj/exp-x/PPOLagLearnedH_0-0_SafetyCarCircle2-v0/SafetyCarCircle2-v0---cc422b1710323180c0538fa59f976c17ef9983ed3f722791683706485ff7ec16/PPOLagLearnedH-{SafetyCarCircle2-v0}/seed-000-2025-02-06-13-32-45'\n",
    "classifier_path = os.path.join(base_path, 'torch_save', 'classifier-100.pt')\n",
    "csv_path = os.path.join(base_path, 'trajs')\n",
    "traj_files = [f for f in os.listdir(csv_path) if os.path.isfile(os.path.join(csv_path, f))]\n",
    "\n",
    "num_safe, num_unsafe = 0, 0\n",
    "safe_csv_path = os.path.join(base_path, 'enriched_trajs', 'safe')\n",
    "unsafe_csv_path = os.path.join(base_path, 'enriched_trajs', 'unsafe')\n",
    "\n",
    "try:\n",
    "    classifier_kwargs = {\n",
    "        'feature_dim': mujoco_safety_gymnasium_dict[env_id]['state_dim'] +\n",
    "                       mujoco_safety_gymnasium_dict[env_id]['action_dim'],\n",
    "        'nb_gru_units': 4,\n",
    "        'batch_size': 128,\n",
    "        'gru_layers': 2,\n",
    "        'mlp_arch': mujoco_safety_gymnasium_dict[env_id]['decoder_arch']}\n",
    "    # if isinstance(classifier_nw_class[pt_model_type], DistributionGRU):\n",
    "    #     classifier_kwargs['loc_offset'] = self._cfgs.model_cfgs.classifier.loc_offset\n",
    "    #     classifier_kwargs['log_std_offset'] = self._cfgs.model_cfgs.classifier.log_std_offset\n",
    "\n",
    "    classifier = DistributionGRU(**classifier_kwargs)\n",
    "    classifier.load_state_dict(torch.load(classifier_path, weights_only=False))\n",
    "except FileNotFoundError as error:\n",
    "    raise FileNotFoundError('The classifier is not found in the save directory.') from error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f059df-72d6-4d07-82e4-b9db5f8a1b3a",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbf38b2-31d0-454c-ba94-3be75bd04c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "Processing file 100\n",
      "Processing file 200\n",
      "Processing file 300\n",
      "Processing file 400\n",
      "Processing file 500\n",
      "Processing file 600\n",
      "Processing file 700\n",
      "Processing file 800\n",
      "Processing file 900\n",
      "Processing file 1000\n",
      "Processing file 1100\n",
      "Processing file 1200\n",
      "Processing file 1300\n",
      "Processing file 1400\n",
      "Processing file 1500\n",
      "Processing file 1600\n",
      "Processing file 1700\n",
      "Processing file 1800\n",
      "Processing file 1900\n",
      "Processing file 2000\n",
      "Processing file 2100\n",
      "Processing file 2200\n",
      "Processing file 2300\n",
      "Processing file 2400\n",
      "Processing file 2500\n",
      "Processing file 2600\n",
      "Processing file 2700\n",
      "Processing file 2800\n",
      "Processing file 2900\n",
      "Processing file 3000\n",
      "Processing file 3100\n",
      "Processing file 3200\n",
      "Processing file 3300\n",
      "Processing file 3400\n",
      "Processing file 3500\n",
      "Processing file 3600\n",
      "Processing file 3700\n",
      "Processing file 3800\n",
      "Processing file 3900\n",
      "Processing file 4000\n",
      "Processing file 4100\n",
      "Processing file 4200\n",
      "Processing file 4300\n",
      "Processing file 4400\n",
      "Processing file 4500\n",
      "Processing file 4600\n",
      "Processing file 4700\n",
      "Processing file 4800\n",
      "Processing file 4900\n",
      "Processing file 5000\n",
      "Processing file 5100\n",
      "Processing file 5200\n",
      "Processing file 5300\n",
      "Processing file 5400\n",
      "Processing file 5500\n",
      "Processing file 5600\n",
      "Processing file 5700\n",
      "Processing file 5800\n",
      "Processing file 5900\n",
      "Processing file 6000\n",
      "Processing file 6100\n",
      "Processing file 6200\n",
      "Processing file 6300\n",
      "Processing file 6400\n",
      "Processing file 6500\n",
      "Processing file 6600\n",
      "Processing file 6700\n",
      "Processing file 6800\n",
      "Processing file 6900\n",
      "Processing file 7000\n",
      "Processing file 7100\n",
      "Processing file 7200\n",
      "Processing file 7300\n",
      "Processing file 7400\n",
      "Processing file 7500\n",
      "Processing file 7600\n",
      "Processing file 7700\n",
      "Processing file 7800\n",
      "Processing file 7900\n",
      "Processing file 8000\n",
      "Processing file 8100\n",
      "Processing file 8200\n",
      "Processing file 8300\n",
      "Processing file 8400\n",
      "Processing file 8500\n",
      "Processing file 8600\n",
      "Processing file 8700\n",
      "Processing file 8800\n",
      "Processing file 8900\n",
      "Processing file 9000\n",
      "Processing file 9100\n",
      "Processing file 9200\n",
      "Processing file 9300\n",
      "Processing file 9400\n",
      "Processing file 9500\n",
      "Processing file 9600\n",
      "Processing file 9700\n",
      "Processing file 9800\n",
      "Processing file 9900\n",
      "Processing file 10000\n",
      "Processing file 10100\n",
      "Processing file 10200\n",
      "Processing file 10300\n",
      "Processing file 10400\n",
      "Processing file 10500\n",
      "Processing file 10600\n",
      "Processing file 10700\n",
      "Processing file 10800\n",
      "Processing file 10900\n",
      "Processing file 11000\n",
      "Processing file 11100\n",
      "Processing file 11200\n",
      "Processing file 11300\n",
      "Processing file 11400\n",
      "Processing file 11500\n",
      "Processing file 11600\n",
      "Processing file 11700\n",
      "Processing file 11800\n",
      "Processing file 11900\n",
      "Processing file 12000\n",
      "Processing file 12100\n",
      "Processing file 12200\n",
      "Processing file 12300\n",
      "Processing file 12400\n",
      "Processing file 12500\n",
      "Processing file 12600\n",
      "Processing file 12700\n",
      "Processing file 12800\n",
      "Processing file 12900\n",
      "Processing file 13000\n",
      "Processing file 13100\n",
      "Processing file 13200\n",
      "Processing file 13300\n",
      "Processing file 13400\n",
      "Processing file 13500\n",
      "Processing file 13600\n",
      "Processing file 13700\n",
      "Processing file 13800\n",
      "Processing file 13900\n",
      "Processing file 14000\n",
      "Processing file 14100\n",
      "Processing file 14200\n",
      "Processing file 14300\n",
      "Processing file 14400\n",
      "Processing file 14500\n",
      "Processing file 14600\n",
      "Processing file 14700\n",
      "Processing file 14800\n",
      "Processing file 14900\n",
      "Processing file 15000\n",
      "Processing file 15100\n",
      "Processing file 15200\n",
      "Processing file 15300\n",
      "Processing file 15400\n",
      "Processing file 15500\n",
      "Processing file 15600\n",
      "Processing file 15700\n",
      "Processing file 15800\n",
      "Processing file 15900\n",
      "Processing file 16000\n",
      "Processing file 16100\n",
      "Processing file 16200\n",
      "Processing file 16300\n",
      "Processing file 16400\n",
      "Processing file 16500\n",
      "Processing file 16600\n",
      "Processing file 16700\n",
      "Processing file 16800\n",
      "Processing file 16900\n",
      "Processing file 17000\n",
      "Processing file 17100\n",
      "Processing file 17200\n",
      "Processing file 17300\n",
      "Processing file 17400\n",
      "Processing file 17500\n",
      "Processing file 17600\n",
      "Processing file 17700\n",
      "Processing file 17800\n",
      "Processing file 17900\n",
      "Processing file 18000\n",
      "Processing file 18100\n",
      "Processing file 18200\n",
      "Processing file 18300\n",
      "Processing file 18400\n",
      "Processing file 18500\n",
      "Processing file 18600\n",
      "Processing file 18700\n",
      "Processing file 18800\n",
      "Processing file 18900\n",
      "Processing file 19000\n",
      "Processing file 19100\n",
      "Processing file 19200\n",
      "Processing file 19300\n",
      "Processing file 19400\n",
      "Processing file 19500\n",
      "Processing file 19600\n",
      "Processing file 19700\n",
      "Processing file 19800\n",
      "Processing file 19900\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for idx, filename in enumerate(traj_files):\n",
    "    if idx % 100 == 0:\n",
    "        print(\"Processing file\", idx)\n",
    "    df = pd.read_csv(os.path.join(csv_path, filename))\n",
    "    orig_obs = torch.tensor(df[[colname for colname in df.columns if colname.startswith('s')]].to_numpy(), dtype=torch.float32)\n",
    "    action = torch.tensor(df[[colname for colname in df.columns if colname.startswith('a')]].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    obs_action = torch.concat((orig_obs, action), dim=-1).unsqueeze(dim=0)\n",
    "    \n",
    "    obs_action = torch.concat((orig_obs, action), dim=-1).unsqueeze(dim=0)\n",
    "    full_hidden_obs = torch.zeros((2, 1, 4))\n",
    "    \n",
    "    prob_feasible, dict_logscores_t, next_hidden_obs_t, next_full_hidden_obs = classifier(\n",
    "        obs_action,\n",
    "        torch.FloatTensor([obs_action.shape[1]] * obs_action.shape[0]),\n",
    "        init_h=full_hidden_obs\n",
    "    )\n",
    "    \n",
    "    logscores_t, mean_logscores_t, var_logscores_t = (\n",
    "        dict_logscores_t['log_scores'], dict_logscores_t['mean'], dict_logscores_t['var']\n",
    "    )\n",
    "    \n",
    "    pred_logscores = mean_logscores_t.squeeze()\n",
    "    \n",
    "    df['new_logscore_mean'] = pred_logscores.detach().numpy()\n",
    "    df['cum_c'] = df['c'].cumsum()\n",
    "\n",
    "    if df['c'].sum() > 25:\n",
    "        os.makedirs(unsafe_csv_path, exist_ok=True)\n",
    "        df.to_csv(os.path.join(unsafe_csv_path, f'traj-{num_unsafe}.csv'), index=False)\n",
    "        num_unsafe += 1\n",
    "    else:\n",
    "        os.makedirs(safe_csv_path, exist_ok=True)\n",
    "        df.to_csv(os.path.join(safe_csv_path, f'traj-{num_safe}.csv'), index=False)\n",
    "        num_safe += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c592d-857a-414e-ad1c-6b791657576f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb42246-e592-4b4c-add9-13147d5e87e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
